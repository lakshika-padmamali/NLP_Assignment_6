{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2446c67f-434d-4a49-91ad-7b58a2aaa9b4",
   "metadata": {},
   "source": [
    "#### Data Processing & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fcfb8a-72fc-49b9-9707-b05f174221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c76722e-a78e-4287-abef-d198330aff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3ef966-f3a5-4cff-bab0-973daa6c1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 9 GitHub repositories from lakshika-padmamali.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Your GitHub username\n",
    "GITHUB_USERNAME = \"lakshika-padmamali\"\n",
    "\n",
    "# GitHub API URL\n",
    "GITHUB_API_URL = f\"https://api.github.com/users/{GITHUB_USERNAME}/repos\"\n",
    "\n",
    "# Fetch repositories\n",
    "response = requests.get(GITHUB_API_URL)\n",
    "repos = response.json()\n",
    "\n",
    "# Select repositories to include\n",
    "repo_names = [repo[\"name\"] for repo in repos]\n",
    "\n",
    "# Function to fetch README content\n",
    "def fetch_readme(repo_name):\n",
    "    url = f\"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{repo_name}/main/README.md\"\n",
    "    readme_response = requests.get(url)\n",
    "    if readme_response.status_code == 200:\n",
    "        return readme_response.text\n",
    "    else:\n",
    "        return f\"README not found for {repo_name}\"\n",
    "\n",
    "# Load repositories as documents\n",
    "github_documents = [\n",
    "    Document(page_content=fetch_readme(repo), metadata={\"source\": f\"GitHub/{repo}\"})\n",
    "    for repo in repo_names\n",
    "]\n",
    "\n",
    "print(f\"✅ Loaded {len(github_documents)} GitHub repositories from {GITHUB_USERNAME}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b953d269-fcd3-481e-861e-3e55dfeb0248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total documents loaded (including GitHub): 23\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader, TextLoader\n",
    "import os\n",
    "\n",
    "data_directory = \"personal_data/\"\n",
    "\n",
    "# Load personal sources\n",
    "personal_sources = [\n",
    "    TextLoader(os.path.join(data_directory, \"Facebook_profile.txt\")).load(),\n",
    "    PyMuPDFLoader(os.path.join(data_directory, \"Linkedin_Profile.pdf\")).load(),\n",
    "    PyMuPDFLoader(os.path.join(data_directory, \"CV.pdf\")).load()\n",
    "]\n",
    "\n",
    "# Merge all documents\n",
    "documents = sum(personal_sources, []) + github_documents\n",
    "\n",
    "print(f\"✅ Total documents loaded (including GitHub): {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed92398-0ebd-478b-9fef-0e576e98360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded and indexed 101 document chunks for retrieval.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Store embeddings in FAISS for retrieval\n",
    "vector_store = FAISS.from_documents(all_splits, embeddings)\n",
    "# Save FAISS index locally\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "print(f\"✅ Loaded and indexed {len(all_splits)} document chunks for retrieval.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa314a20-3c53-4ed9-97d2-c1bd044950c7",
   "metadata": {},
   "source": [
    "#### Load the FAISS Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58710e94-f2e8-446e-bfa3-7d6ac988a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13781/1056035348.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(question, k=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Source: personal_data/CV.pdf\n",
      "🔹 Content:\n",
      "research, aligning seamlessly with my commitment to innovative methodologies \n",
      "LAKSHIKA PADMAMALI\n",
      "ELECTRICAL AND INFORMATION ENGINEERING\n",
      "GRADUATE\n",
      " \n",
      "Bachelor of Science in Electrical and Information Engineering\n",
      "University of Ruhuna, Sri Lanka | 2017-2023\n",
      "Second Class Upper Division\n",
      "Overall Grade Point Average (OGPA): 3.61\n",
      "Relevant Modules - Computer Vision and Image Processing, Machine Learning, Information\n",
      "Security, Intelligence System Design, Applications in Biomedical Engineering, Analog\n",
      "Electr\n",
      "\n",
      "📄 Source: personal_data/CV.pdf\n",
      "🔹 Content:\n",
      "landscape.\n",
      "LAKSHIKA PADMAMALI\n",
      "ELECTRICAL AND INFORMATION ENGINEERING\n",
      "GRADUATE\n",
      " \n",
      "Bachelor of Science in Electrical and Information Engineering\n",
      "University of Ruhuna, Sri Lanka | 2017-2023\n",
      "Second Class Upper Division\n",
      "Overall Grade Point Average (OGPA): 3.61\n",
      "Relevant Modules - Computer Vision and Image Processing, Machine Learning, Information\n",
      "Security, Intelligence System Design, Applications in Biomedical Engineering, Analog\n",
      "Electronics, Digital Electronics, Computer Architecture, Embedded System \n",
      "\n",
      "📄 Source: personal_data/CV.pdf\n",
      "🔹 Content:\n",
      "aligning seamlessly with my commitment to innovative methodologies \n",
      "LAKSHIKA PADMAMALI\n",
      "ELECTRICAL AND INFORMATION ENGINEERING\n",
      "GRADUATE\n",
      " \n",
      "Bachelor of Science in Electrical and Information Engineering\n",
      "University of Ruhuna, Sri Lanka | 2017-2023\n",
      "Second Class Upper Division\n",
      "Overall Grade Point Average (OGPA): 3.61\n",
      "Relevant Modules - Computer Vision and Image Processing, Machine Learning, Information\n",
      "Security, Intelligence System Design, Applications in Biomedical Engineering, Analog\n",
      "Electronics, Dig\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Load FAISS retriever inside the notebook\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load FAISS index safely\n",
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_index\",  # Path where FAISS is stored\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Explicitly allow loading pickle\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Test retrieval\n",
    "question = \"What is your field of study?\"\n",
    "results = retriever.get_relevant_documents(question, k=3)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"📄 Source: {doc.metadata['source']}\\n🔹 Content:\\n{doc.page_content[:500]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ecd98-1710-430a-8745-4dec64685961",
   "metadata": {},
   "source": [
    "#### Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db36ed1-0ddd-49be-977b-ae8fd4479ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Answer: My highest level of education is a Bachelor of Science in Electrical and Information Engineering from the University of Ruhuna in Sri Lanka. In addition, I have pursued a GIS Postgraduate Course at the Postgraduate Institute of Science, University of Peradeniya, Sri Lanka, and a Diploma in English at the Sabaragamwa University of Sri Lanka.\n",
      "📄 Source: personal_data/CV.pdf\n",
      "📄 Source: personal_data/CV.pdf\n",
      "📄 Source: personal_data/CV.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Load GPT-4 for testing\n",
    "llm_gpt4 = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Load Groq LLaMA3-70B for final results\n",
    "llm_llama3 = ChatGroq(model_name=\"llama3-70b-8192\", groq_api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Load FAISS Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load FAISS retriever\n",
    "#embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "#vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "#retriever = vector_store.as_retriever()\n",
    "\n",
    "# Function to retrieve relevant documents\n",
    "def retrieve_documents(question, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents from the FAISS index.\n",
    "    \"\"\"\n",
    "    results = retriever.get_relevant_documents(question, k=top_k)\n",
    "    return results\n",
    "\n",
    "# Generate chatbot response\n",
    "def generate_response(question, use_groq=False):\n",
    "    \"\"\"\n",
    "    Generate a chatbot response using either GPT-4 (for testing) or Groq (for final results).\n",
    "    \"\"\"\n",
    "    relevant_docs = retrieve_documents(question)\n",
    "    if relevant_docs:\n",
    "        context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    else:\n",
    "        context = None  # Handle case where no documents are found\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant answering questions about {os.getenv(\"USERNAME\", \"the user\")}.\n",
    "    Be professional, friendly, and provide informative answers. You are an AI assistant providing responses as if you were the user.\n",
    "    Respond in the first person (I, my) instead of using the user's full name.\n",
    "\n",
    "    {\"Use the following context to answer the question:\\n\\n\" + context if context else \"There is no relevant information available, so provide a general response based on common knowledge.\"}\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Use GPT-4 for debugging, Groq for final results\n",
    "    if use_groq:\n",
    "        response = llm_llama3.invoke(prompt)\n",
    "    else:\n",
    "        response = llm_gpt4.invoke(prompt)\n",
    "\n",
    "    return response.content, relevant_docs\n",
    "\n",
    "# Example Test\n",
    "question = \"What is your highest level of education?\"\n",
    "answer, sources = generate_response(question)\n",
    "\n",
    "# Display the response\n",
    "print(f\"💬 Answer: {answer}\")\n",
    "\n",
    "# Print sources if available\n",
    "if sources:\n",
    "    for src in sources:\n",
    "        print(f\"📄 Source: {src.metadata['source']}\")\n",
    "else:\n",
    "    print(\"⚠️ No relevant sources found. AI generated a general response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236955d-1995-4a80-843c-da3fb3af1557",
   "metadata": {},
   "source": [
    "#### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0440881b-9b07-4b30-bc33-78555e544672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Performance analysis saved to chatbot_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# List of questions\n",
    "questions = [\n",
    "    \"How old are you?\",\n",
    "    \"What is your highest level of education?\",\n",
    "    \"What major or field of study did you pursue during your education?\",\n",
    "    \"How many years of work experience do you have?\",\n",
    "    \"What type of work or industry have you been involved in?\",\n",
    "    \"Can you describe your current role or job responsibilities?\",\n",
    "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
    "    \"How do you think cultural values should influence technological advancements?\",\n",
    "    \"As a master’s student, what is the most challenging aspect of your studies so far?\",\n",
    "    \"What specific research interests or academic goals do you hope to achieve during your time as a master’s student?\"\n",
    "]\n",
    "\n",
    "\n",
    "# Evaluate both models\n",
    "results = {\"GPT-4\": [], \"Groq LLaMA3-70B\": []}\n",
    "\n",
    "for question in questions:\n",
    "    gpt4_answer, _ = generate_response(question, use_groq=False)\n",
    "    groq_answer, _ = generate_response(question, use_groq=True)\n",
    "\n",
    "    results[\"GPT-4\"].append({\"question\": question, \"answer\": gpt4_answer})\n",
    "    results[\"Groq LLaMA3-70B\"].append({\"question\": question, \"answer\": groq_answer})\n",
    "\n",
    "# Save results in JSON format\n",
    "with open(\"chatbot_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"✅ Performance analysis saved to chatbot_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa63ac68-1510-4695-bc89-008d09697070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelListResponse(data=[Model(id='deepseek-r1-distill-qwen-32b', created=1738891590, object='model', owned_by='DeepSeek / Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None), Model(id='qwen-2.5-coder-32b', created=1739494572, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='qwen-qwq-32b', created=1741214760, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mistral-saba-24b', created=1739996492, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='qwen-2.5-32b', created=1738789898, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None)], object='list')\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "# Fetch the list of available models\n",
    "available_models = client.models.list()\n",
    "\n",
    "# Print the response to understand its structure\n",
    "print(available_models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b230a12-a388-451b-9036-a35f89745df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c795a6d-11d0-4ce4-9daf-08e7a49c3780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
